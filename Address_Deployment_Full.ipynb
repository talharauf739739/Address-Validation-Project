{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6jUHsGGs0Hw",
        "outputId": "0e1b5ea4-438f-42fa-b62a-d75575be30ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9980376766091051\n",
            "Precision: 0.9980422758045526\n",
            "ROC AUC: 0.9981952339397528\n",
            "R1-score: 0.9980376766091051\n",
            "Best Parameters: {'colsample_bytree': 0.7, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.7}\n",
            "Training Completed and Model Saved as Address_Validator_Model.pkl\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "import pickle\n",
        "import regex as re\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, precision_score, roc_curve, auc, recall_score\n",
        "\n",
        "# Load the training data containing 'address', 'city', 'province', and 'Result' columns\n",
        "training_data = pd.read_excel('/content/Output_Model.xlsx')\n",
        "\n",
        "# Concatenate 'address', 'city', and 'province' columns into a single 'full_address' column\n",
        "training_data['full_address'] = training_data['address'].astype(str) + ', ' + training_data['city'].astype(str) + ', ' + training_data['province'].astype(str)\n",
        "\n",
        "# Define the replacements for the address column\n",
        "replacements = {\n",
        "    r'\\bH\\b': 'House',\n",
        "    r'\\bh\\b': 'house',\n",
        "    r'\\bH(?![oO])\\b': 'House',\n",
        "    r'\\bh(?![oO])\\b': 'house',\n",
        "    r'\\bst\\b': 'Street',\n",
        "    r'\\bSt\\b': 'Street',\n",
        "    r'\\bST\\b': 'Street',\n",
        "    r'\\bST(?![rR])\\b': 'Street',\n",
        "    r'\\bSt(?![rR])\\b': 'Street',\n",
        "    r'\\bst(?![rR])\\b': 'Street'\n",
        "}\n",
        "\n",
        "# Function to perform replacements\n",
        "def replace_words(text):\n",
        "    if isinstance(text, str):\n",
        "        for pattern, replacement in replacements.items():\n",
        "            text = re.sub(pattern, replacement, text)\n",
        "        return text\n",
        "    else:\n",
        "        return text\n",
        "\n",
        "# Apply replacements to each string in the 'full_address' column\n",
        "training_data['updated_address'] = training_data['full_address'].apply(replace_words)\n",
        "\n",
        "# Define feature extraction functions\n",
        "def extract_feature_1(text):\n",
        "    return 'Sector' in text or 'sector' in text or 'SECTOR' in text or 'Block' in text or 'BLOCK' in text or 'block' in text or 'House' in text or 'HOUSE' in text or 'house' in text or 'Street' in text or 'STREET' in text or 'street' in text or 'FLAT' in text or 'flat' in text or 'Flat' in text\n",
        "#'Sector|sector|SECTOR|Block|block|BLOCK|House|house|HOUSE|Street|street|STREET|Flat|flat|FLAT'\n",
        "\n",
        "def extract_feature_2(text):\n",
        "    return any(char.isdigit() for char in text)\n",
        "\n",
        "def extract_feature_3(text):\n",
        "    return re.search(r'(House|house|HOUSE|H|no|NO|No|number|Number|n|N|Street|St|st|street|STREET) \\d', text) is not None\n",
        "\n",
        "# Extract features and add them as columns\n",
        "training_data['Featured_1'] = training_data['updated_address'].apply(extract_feature_1)\n",
        "training_data['Featured_2'] = training_data['updated_address'].apply(extract_feature_2)\n",
        "training_data['Featured_3'] = training_data['updated_address'].apply(extract_feature_3)\n",
        "\n",
        "# Use label encoding to convert textual labels to numerical values for the 'Result' column\n",
        "label_encoder = LabelEncoder()\n",
        "training_data['Result'] = label_encoder.fit_transform(training_data['Result'])\n",
        "\n",
        "# Drop unwanted columns\n",
        "training_data.drop(['order_id', 'province', 'city', 'address', 'status', 'updated_address', 'full_address'], axis=1, inplace=True)\n",
        "\n",
        "# Separate features (X) and target (y)\n",
        "X = training_data.drop('Result', axis=1)\n",
        "y = training_data['Result']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the XGBoost model\n",
        "xgb_model = xgb.XGBClassifier(\n",
        "    learning_rate=0.01,\n",
        "    n_estimators=500,\n",
        "    max_depth=300,\n",
        "    subsample=0.85,\n",
        "    colsample_bytree=0.85,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Define the hyperparameters to search\n",
        "param_grid = {\n",
        "    'max_depth': [3, 6],\n",
        "    'n_estimators': [100, 300],\n",
        "    'learning_rate': [0.01, 0.1],\n",
        "    'subsample': [0.7, 0.8],\n",
        "    'colsample_bytree': [0.7, 0.8]\n",
        "}\n",
        "\n",
        "# Create a GridSearchCV object\n",
        "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, scoring='accuracy', cv=5)\n",
        "\n",
        "# Perform the grid search\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters and estimator\n",
        "best_params = grid_search.best_params_\n",
        "best_estimator = grid_search.best_estimator_\n",
        "\n",
        "# Train the model with the best parameters\n",
        "best_estimator.fit(X_train, y_train)\n",
        "\n",
        "# Predict using the model\n",
        "y_pred_encoded = best_estimator.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred_encoded)\n",
        "\n",
        "# Calculate precision\n",
        "precision = precision_score(y_test, y_pred_encoded, average='weighted')\n",
        "\n",
        "# Calculate ROC curve and AUC\n",
        "y_pred_prob = best_estimator.predict_proba(X_test)[:, 1]\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Calculate R1-score\n",
        "recall = recall_score(y_test, y_pred_encoded, average='weighted')\n",
        "\n",
        "# Print the metrics\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"ROC AUC:\", roc_auc)\n",
        "print(\"R1-score:\", recall)\n",
        "\n",
        "# Save the trained model to a file\n",
        "model_filename = 'Address_Validator_Model.pkl'\n",
        "with open(model_filename, 'wb') as model_file:\n",
        "    pickle.dump(best_estimator, model_file)\n",
        "\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Training Completed and Model Saved as Address_Validator_Model.pkl\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Below Prediction Stuff**"
      ],
      "metadata": {
        "id": "j5o3yytY6T0N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Load the trained XGBoost model\n",
        "model_filename = 'Address_Validator_Model.pkl'\n",
        "with open(model_filename, 'rb') as model_file:\n",
        "    loaded_model = pickle.load(model_file)\n",
        "\n",
        "# Define the input strings\n",
        "address = 'Soft Swirl ice cream shop'\n",
        "province = 'Federal'\n",
        "city = 'Islamabad'\n",
        "\n",
        "# Concatenate 'address', 'province', and 'city' into a single string\n",
        "full_address = f\"{address}, {city}, {province}\"\n",
        "\n",
        "# Apply replacements to the 'full_address' string (similar to the preprocessing steps)\n",
        "updated_address = replace_words(full_address)\n",
        "\n",
        "# Extract features and add them as columns (similar to the preprocessing steps)\n",
        "featured_1 = extract_feature_1(updated_address)\n",
        "featured_2 = extract_feature_2(updated_address)\n",
        "featured_3 = extract_feature_3(updated_address)\n",
        "\n",
        "# Create a DataFrame with the extracted features\n",
        "input_data = pd.DataFrame({\n",
        "    'Featured_1': [featured_1],\n",
        "    'Featured_2': [featured_2],\n",
        "    'Featured_3': [featured_3]\n",
        "})\n",
        "\n",
        "# Make predictions using the loaded model\n",
        "y_pred_encoded = loaded_model.predict(input_data)\n",
        "\n",
        "# Inverse transform the predicted labels to get the original textual class labels\n",
        "predicted_labels = label_encoder.inverse_transform(y_pred_encoded)\n",
        "\n",
        "print(\"Predicted Label:\", predicted_labels[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RqB4rvzjvLTa",
        "outputId": "704ff969-7a09-48da-b88e-8a2ae4c0e9a1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Label: Incomplete\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zHJwBNfizQmy"
      },
      "execution_count": 14,
      "outputs": []
    }
  ]
}